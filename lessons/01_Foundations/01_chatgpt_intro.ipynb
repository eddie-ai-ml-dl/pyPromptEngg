{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab12abce",
   "metadata": {},
   "source": [
    "## ChatGPT API Introduction\n",
    "\n",
    "This notebook introduces the OpenAI ChatGPT API for building LLM-powered applications. We'll explore different ways to interact with the API, from basic prompts to multi-turn conversations with context management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e2c2df",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "\n",
    "Import the OpenAI client and helper functions for interacting with the ChatGPT API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:29:40.510161Z",
     "start_time": "2025-04-14T00:29:40.260161Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from src.openai_client import list_text_models,generate_text\n",
    "from src.openai_client import Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "914314cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-3.5-turbo',\n",
       " 'gpt-3.5-turbo-0125',\n",
       " 'gpt-3.5-turbo-1106',\n",
       " 'gpt-3.5-turbo-16k',\n",
       " 'gpt-3.5-turbo-instruct',\n",
       " 'gpt-3.5-turbo-instruct-0914',\n",
       " 'gpt-4',\n",
       " 'gpt-4-0125-preview',\n",
       " 'gpt-4-0613',\n",
       " 'gpt-4-1106-preview',\n",
       " 'gpt-4-turbo',\n",
       " 'gpt-4-turbo-2024-04-09',\n",
       " 'gpt-4-turbo-preview',\n",
       " 'gpt-4.1',\n",
       " 'gpt-4.1-2025-04-14',\n",
       " 'gpt-4.1-mini',\n",
       " 'gpt-4.1-mini-2025-04-14',\n",
       " 'gpt-4.1-nano',\n",
       " 'gpt-4.1-nano-2025-04-14',\n",
       " 'gpt-4o',\n",
       " 'gpt-4o-2024-05-13',\n",
       " 'gpt-4o-2024-08-06',\n",
       " 'gpt-4o-2024-11-20',\n",
       " 'gpt-4o-mini',\n",
       " 'gpt-4o-mini-2024-07-18',\n",
       " 'gpt-4o-mini-realtime-preview',\n",
       " 'gpt-4o-mini-realtime-preview-2024-12-17',\n",
       " 'gpt-4o-mini-search-preview',\n",
       " 'gpt-4o-mini-search-preview-2025-03-11',\n",
       " 'gpt-4o-mini-transcribe',\n",
       " 'gpt-4o-mini-transcribe-2025-03-20',\n",
       " 'gpt-4o-mini-transcribe-2025-12-15',\n",
       " 'gpt-4o-realtime-preview',\n",
       " 'gpt-4o-realtime-preview-2024-12-17',\n",
       " 'gpt-4o-realtime-preview-2025-06-03',\n",
       " 'gpt-4o-search-preview',\n",
       " 'gpt-4o-search-preview-2025-03-11',\n",
       " 'gpt-4o-transcribe',\n",
       " 'gpt-4o-transcribe-diarize',\n",
       " 'gpt-5',\n",
       " 'gpt-5-2025-08-07',\n",
       " 'gpt-5-chat-latest',\n",
       " 'gpt-5-codex',\n",
       " 'gpt-5-mini',\n",
       " 'gpt-5-mini-2025-08-07',\n",
       " 'gpt-5-nano',\n",
       " 'gpt-5-nano-2025-08-07',\n",
       " 'gpt-5-pro',\n",
       " 'gpt-5-pro-2025-10-06',\n",
       " 'gpt-5-search-api',\n",
       " 'gpt-5-search-api-2025-10-14',\n",
       " 'gpt-5.1',\n",
       " 'gpt-5.1-2025-11-13',\n",
       " 'gpt-5.1-chat-latest',\n",
       " 'gpt-5.1-codex',\n",
       " 'gpt-5.1-codex-max',\n",
       " 'gpt-5.1-codex-mini',\n",
       " 'gpt-5.2',\n",
       " 'gpt-5.2-2025-12-11',\n",
       " 'gpt-5.2-chat-latest',\n",
       " 'gpt-5.2-pro',\n",
       " 'gpt-5.2-pro-2025-12-11',\n",
       " 'gpt-realtime',\n",
       " 'gpt-realtime-2025-08-28',\n",
       " 'gpt-realtime-mini',\n",
       " 'gpt-realtime-mini-2025-10-06',\n",
       " 'gpt-realtime-mini-2025-12-15',\n",
       " 'o1',\n",
       " 'o1-2024-12-17',\n",
       " 'o1-pro',\n",
       " 'o1-pro-2025-03-19',\n",
       " 'o3',\n",
       " 'o3-2025-04-16',\n",
       " 'o3-mini',\n",
       " 'o3-mini-2025-01-31',\n",
       " 'o4-mini',\n",
       " 'o4-mini-2025-04-16',\n",
       " 'o4-mini-deep-research',\n",
       " 'o4-mini-deep-research-2025-06-26',\n",
       " 'omni-moderation-2024-09-26',\n",
       " 'omni-moderation-latest']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_text_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f1b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available text models using helper\n",
    "from src.openai_client import list_text_models\n",
    "models = list_text_models()\n",
    "print(f\"Found {len(models)} text-capable models:\")\n",
    "for m in models:\n",
    "    print(\"-\", m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6527be8",
   "metadata": {},
   "source": [
    "### Step 2: Basic Chat Completion\n",
    "\n",
    "Use the OpenAI client to make a simple API call with a system message (pirate persona) and user query. The API returns the assistant's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d496d0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, matey! If ye be wantin' to check if a Python object be an instance of a particular class, ye'll be usin' the `isinstance()` function. Here be a simple example to guide yer ship through the stormy seas of Python coding:\n",
      "\n",
      "```python\n",
      "# First, define yer class, say a vessel class:\n",
      "class Vessel:\n",
      "    pass\n",
      "\n",
      "# Create yerself an object or two:\n",
      "black_pearl = Vessel()\n",
      "\n",
      "# Now check if the object be an instance of the class:\n",
      "if isinstance(black_pearl, Vessel):\n",
      "    print(\"Aye, the Black Pearl be a proper vessel!\")\n",
      "else:\n",
      "    print(\"Arrr, she's no vessel I know!\")\n",
      "\n",
      "# Yarrr! Ye can also check against multiple classes using a tuple:\n",
      "class GhostShip:\n",
      "    pass\n",
      "\n",
      "davy_jones_locker = GhostShip()\n",
      "\n",
      "if isinstance(davy_jones_locker, (Vessel, GhostShip)):\n",
      "    print(\"It be either a vessel or a ghost ship, aye!\")\n",
      "else:\n",
      "    print(\"This be somethin' else entirely!\")\n",
      "```\n",
      "\n",
      "That `isinstance()` function be mighty useful fer checkin' if yer object belongs to a class, or any subclass thereof. So hoist the sails and let yer code sail swift and true on the Python seas! Arrr! üè¥‚Äç‚ò†Ô∏è\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"Talk like a pirate.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How do I check if a Python object is an instance of a class?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce89a45",
   "metadata": {},
   "source": [
    "### Step 3: Using Helper Functions\n",
    "\n",
    "Use the `generate_text()` helper function to simplify API calls. This abstracts away the client setup and handles the completion request internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "316c55ed1d17bce9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T00:40:50.243372Z",
     "start_time": "2025-01-15T00:40:50.240749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      "The highest mountain in the USA is Denali, also known as Mount McKinley. It is located in Alaska and has an elevation of 20,310 feet (6,190 meters) above sea level.\n"
     ]
    }
   ],
   "source": [
    "prompt=\"What's the highest mountain in the USA?\"\n",
    "\n",
    "result = generate_text(prompt=prompt)\n",
    "print(f\"Result:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89017fc6",
   "metadata": {},
   "source": [
    "### Step 4: Another Simple Prompt\n",
    "\n",
    "Demonstrate using the helper function with a different prompt to show its flexibility and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5afc07e961acddd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T00:43:09.799748Z",
     "start_time": "2025-01-15T00:43:08.922718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the SJU student bring a ladder to class?\n",
      "\n",
      "Because they heard the course was about \"higher education!\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me a joke about SJU.\"\n",
    "response = generate_text(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ac38e",
   "metadata": {},
   "source": [
    "### Step 5: Multi-turn Conversation\n",
    "\n",
    "Initialize a `Chat` object that maintains conversation history internally, allowing the model to remember context across multiple messages for coherent multi-turn dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bf47ed45f70d9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's wonderful! Dogs can bring a lot of joy and companionship. What breeds are they, and how do they get along?\n",
      "If you have two dogs, and assuming each dog has four paws, that would be a total of 8 paws. So, there are 8 paws in your house. If you have any other pets or animals, let me know, and we can adjust that number!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the Chat object (manages history internally)\n",
    "chat = Chat(\n",
    "    history=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Great to meet you. What would you like to know?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Send messages\n",
    "response = chat.send_message(\"I have 2 dogs in my house.\")\n",
    "print(response)\n",
    "\n",
    "response = chat.send_message(\"How many paws are in my house?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f317db",
   "metadata": {},
   "source": [
    "### Step 6: Advanced Prompt Engineering\n",
    "\n",
    "Structure a prompt with system context, examples, and detailed instructions using multiple message roles. This demonstrates how to guide the model's behavior through well-crafted prompts and token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49413d80fa2fd64c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T22:52:34.392051Z",
     "start_time": "2024-09-16T22:52:34.375598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Absolutely! Here‚Äôs something to spark that inspiration: Remember, every setback is just a setup for a comeback. Keep pushing forward, because your greatest achievements often lie just beyond your biggest challenges! Let‚Äôs make today count!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.openai_client import get_client\n",
    "\n",
    "client = get_client()\n",
    "\n",
    "context = \"\"\"\n",
    "You are an assistant that speaks like a radio host.\n",
    "\"\"\"\n",
    "examples = \"\"\"\n",
    "When you change the way you look at things,\n",
    "the things you look at will change.\n",
    "\"\"\"\n",
    "prompt = \"\"\"\n",
    "Tell me something inspiring.\n",
    "\"\"\"\n",
    "\n",
    "tokens = 50\n",
    "messages =  [\n",
    "    {'role':'system', 'content':f\"{context}\"},\n",
    "    {'role':'user', 'content':f'{prompt}'},\n",
    "    {'role':'assistant', 'content':f'{examples}'},\n",
    "    # Note: Instructions usually come from system or user, but keeping structure as requested\n",
    "    {'role':'assistant', 'content':f\"Be realistic and practical. Keep your answers to less than {tokens} tokens.\"},\n",
    "    {'role':'user', 'content':'I expect to be inspired!'}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7dcbc015e14810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

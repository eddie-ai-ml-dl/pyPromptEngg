{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCvtxtyyCiYG"
   },
   "source": [
    "## Chat Models - <a href='https://python.langchain.com/docs/modules/data_connection/document_loaders/'>Document Loaders</a> and Text Splitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This notebook saves the fetched README into a temporary folder (tmp/README.md) to avoid overwriting local files. The tmp folder is ignored by Git via .gitignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T22:15:10.724280Z",
     "start_time": "2025-04-01T22:15:10.195367Z"
    },
    "id": "IEvhajVQCiYJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from src.fnUtils import render_markdown\n",
    "\n",
    "# Configure a downloadable text resource (swap this URL as needed)\n",
    "source_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "# Create a temp folder relative to this notebook directory\n",
    "TMP_DIR = \"tmp\"\n",
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "\n",
    "# Derive a sensible local filename from the URL path\n",
    "parsed = urlparse(source_url)\n",
    "file_name = os.path.basename(parsed.path) or \"download.txt\"\n",
    "local_path = os.path.join(TMP_DIR, file_name)\n",
    "\n",
    "# Fetch raw text content\n",
    "resp = requests.get(source_url, timeout=20, headers={\"User-Agent\": \"LangChain-DocLoader-Demo/1.0\"})\n",
    "resp.raise_for_status()\n",
    "text = resp.text\n",
    "\n",
    "# Write to local file\n",
    "with open(local_path, \"w\", encoding=resp.encoding or \"utf-8\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "# Load the saved file as a LangChain document\n",
    "tloader = TextLoader(local_path)\n",
    "docs = tloader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T22:15:18.305635Z",
     "start_time": "2025-04-01T22:15:18.297204Z"
    },
    "id": "p7rHaDryCiYK"
   },
   "outputs": [],
   "source": [
    "# Split the text into chunks for downstream tasks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Small chunk size just to demonstrate splitting\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Use the docs we already loaded from the TextLoader\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# Add chunk indices to metadata for easier citation\n",
    "final_docs = []\n",
    "for idx, doc in enumerate(split_docs):\n",
    "    doc.metadata[\"chunk_index\"] = idx\n",
    "    final_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T22:15:20.678971Z",
     "start_time": "2025-04-01T22:15:20.674265Z"
    },
    "id": "EVpwwheFCiYK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5318"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loader_metadata': {'source': 'tmp/input.txt'}}\n",
      "{'chunk_metadata': {'source': 'tmp/input.txt', 'chunk_index': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Inspect metadata on the loaded doc and the first chunk\n",
    "print({\"loader_metadata\": docs[0].metadata})\n",
    "print({\"chunk_metadata\": final_docs[0].metadata})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T22:15:23.363425Z",
     "start_time": "2025-04-01T22:15:23.350854Z"
    },
    "id": "XJt0jvzqCiYK"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model_name = \"gpt-4o-mini\"  # e.g., \"gpt-4o\" for higher quality\n",
    "\n",
    "chat = ChatOpenAI(model=model_name, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T22:17:23.378588Z",
     "start_time": "2025-04-01T22:17:15.215898Z"
    },
    "id": "PRqOsYmgCiYK"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Map step: summarize each chunk\n",
    "summary_prompt = PromptTemplate.from_template(\n",
    "    \"You are a helpful assistant. Summarize the following passage concisely.\\n\\n{context}\"\n",
    ")\n",
    "map_chain = summary_prompt | chat | StrOutputParser()\n",
    "partial_summaries = map_chain.batch([\n",
    "    {\"context\": d.page_content} for d in final_docs[:10]\n",
    "])\n",
    "\n",
    "# Reduce step: combine and refine partial summaries\n",
    "reduce_prompt = PromptTemplate.from_template(\n",
    "    \"Combine and refine the following partial summaries into a single clear summary.\\n\\n{context}\"\n",
    ")\n",
    "reduce_chain = reduce_prompt | chat | StrOutputParser()\n",
    "result = reduce_chain.invoke({\n",
    "    \"context\": \"\\n\\n\".join(partial_summaries)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> The First Citizen addresses the crowd, urging them to consider their dire situation and their willingness to die rather than starve. The citizens express their determination and identify Caius Marcius as their chief enemy, eager to take swift action to control corn prices. The First Citizen voices frustration over the patricians' perception of the citizens as unworthy and costly, suggesting that sharing their excess resources would demonstrate true concern for the less fortunate. He highlights the injustice of their suffering benefiting the wealthy and advocates for action to address this inequality.\n",
       "> \n",
       "> While some citizens express a desire for vengeance against Marcius, the Second Citizen raises concerns about his past services to the country. The First Citizen acknowledges the complexity of Marcius's character, suggesting that his actions were driven by personal pride rather than genuine patriotism. The Second Citizen counters that inherent traits should not be labeled as vices, particularly greed. As tensions rise, the First Citizen urges the group to move to the Capitol, but he hesitates, sensing unrest and questioning who is approaching."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the final summary\n",
    "render_markdown(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Use-Cases for Long Text (Tiny Shakespeare)\n",
    "Summarization is useful, but for literary corpora you often want:\n",
    "- Retrieval Q&A with citations (answer questions from the text).\n",
    "- Character and entity extraction (who speaks, where, to whom).\n",
    "- Concordance/search (find lines containing themes or motifs).\n",
    "- Scene/section segmentation and study guides with references.\n",
    "Below we add a lightweight retrieval-QA example without embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x118467ce0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a lightweight BM25 retriever from the chunks (no embeddings needed)\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(final_docs)\n",
    "bm25_retriever.k = 4  # top-k chunks to retrieve\n",
    "bm25_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " The main characters introduced early in the text are Derby, Lord Clifford, Lord Stafford, and Duke Vincentio.\n",
      "\n",
      "Citations:\n",
      "  - tmp/input.txt (chunk #1462)\n",
      "  - tmp/input.txt (chunk #2750)\n",
      "  - tmp/input.txt (chunk #498)\n",
      "  - tmp/input.txt (chunk #4496)\n"
     ]
    }
   ],
   "source": [
    "# Ask a question, retrieve top chunks, and answer with citations\n",
    "question = \"Who are the main characters introduced early in the text?\"\n",
    "retrieved = bm25_retriever.invoke(question)\n",
    "context = \"\\n\\n\".join([d.page_content for d in retrieved])\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "qa_prompt = PromptTemplate.from_template(\n",
    "    \"Answer the user's question using only the provided context.\\n\"\n",
    "    \"If unsure, say you don't know.\\n\\n\"\n",
    "    \"Question: {question}\\n\\nContext:\\n{context}\"\n",
    ")\n",
    "qa_chain = qa_prompt | chat | StrOutputParser()\n",
    "answer = qa_chain.invoke({\"question\": question, \"context\": context})\n",
    "print(\"Answer:\\n\", answer)\n",
    "print(\"\\nCitations:\")\n",
    "for doc in retrieved:\n",
    "    source = doc.metadata.get(\"source\", \"unknown\")\n",
    "    chunk_idx = doc.metadata.get(\"chunk_index\", \"N/A\")\n",
    "    print(f\"  - {source} (chunk #{chunk_idx})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

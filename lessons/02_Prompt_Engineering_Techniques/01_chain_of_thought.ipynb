{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain of Thought\n",
    "\n",
    "Chain of Thought (CoT) prompting is a technique for eliciting reasoning in LLMs, by asking the model to think first before completing a task. CoT prompting has shown significant improvements in the performance of language models on tasks that require multi-step reasoning, such as math word problems, commonsense reasoning, and symbolic manipulation. By providing a step-by-step thought process, the model can better understand and solve complex problems. This technique enhances the reasoning capabilities of large language models by encouraging them to break down complex problems into intermediate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n",
    "\n",
    "The CoT technique has been referenced in many AI papers since, but was first introduced by Jason Wei et al. The authors demonstrate that CoT prompting is particularly effective in few-shot learning scenarios and that its benefits scale with model size. The technique shows versatility across various tasks. Importantly, CoT prompting makes the model's reasoning process more transparent and human-like, enhancing interpretability. The process is analogous to system 2 thinking in the human brain, when a problem is solved by spending time thinking a more deliberate and structured way, as opposed to system 1 thinking which is more intuitive and instinctive (like LLMs are without CoT). The paper's findings suggest that future AI developments might focus more on prompting techniques to unlock latent abilities in existing models, rather than solely on developing larger models. While highly effective, the authors note that CoT prompting has limitations and may not be as beneficial for simpler tasks or very small models.\n",
    "\n",
    "> [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) by Wei, J., et al. (2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T19:31:32.364777Z",
     "start_time": "2025-02-05T19:31:28.546913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Prompt Result:\n",
      "Q: How many Rs are there in RASPBERRY?\n",
      "A: 2\n"
     ]
    }
   ],
   "source": [
    "from src.openai_client import generate_text\n",
    "\n",
    "\n",
    "system = \"Solve the following problem and return the answer in the format A: <answer>\"\n",
    "# Standard prompting (few-shot)\n",
    "question = \"\"\"Q: How many Rs are there in RASPBERRY?\"\"\"\n",
    "\n",
    "standard_prompt = f\"\"\"Q: How many Es are there in ELEPHANT?\n",
    "A: 2\n",
    "Q: How many Ps are there in PINEAPPLE?\n",
    "A: 3\n",
    "Q: How many Os are there in CHOCOLATE?\n",
    "A: 2\n",
    "{question}\"\"\"\n",
    "\n",
    "output = generate_text(standard_prompt, system_instruction=system)\n",
    "print(f\"Standard Prompt Result:\\n{question}\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T19:34:32.766789Z",
     "start_time": "2025-02-05T19:34:31.338648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chain of Thought Prompt Result:\n",
      "Spell out RASPBERRY and count the Rs:\n",
      "R-A-S-P-B-E-R-R-Y\n",
      "There's one R at the beginning and two Rs together at the end. 1 + 2 = 3 Rs in total.\n",
      "A: 3\n"
     ]
    }
   ],
   "source": [
    "# Chain of Thought prompting (zero-shot)\n",
    "cot_prompt = f\"\"\"Q: How many Es are there in ELEPHANT?\n",
    "Let's think step by step:\n",
    "Spell out ELEPHANT and count the Es:\n",
    "E-L-E-P-H-A-N-T\n",
    "There's one E at the beginning and one in the middle. 1 + 1 = 2 Es in total.\n",
    "A: 2\n",
    "\n",
    "Q: How many Ps are there in PINEAPPLE?\n",
    "Let's think step by step:\n",
    "Spell out PINEAPPLE and count the Ps:\n",
    "P-I-N-E-A-P-P-L-E\n",
    "There's one P at the beginning and two Ps together in the middle. 1 + 2 = 3 Ps in total.\n",
    "A: 3\n",
    "\n",
    "Q: How many Os are there in CHOCOLATE?\n",
    "Let's think step by step:\n",
    "Spell out CHOCOLATE and count the Os:\n",
    "C-H-O-C-O-L-A-T-E\n",
    "There's one O at the beginning and one in the middle. 1 + 1 = 2 Os in total.\n",
    "A: 2\n",
    "\n",
    "{question}.\n",
    "Let's think step by step:\"\"\"\n",
    "\n",
    "print(\"\\nChain of Thought Prompt Result:\")\n",
    "print(generate_text(cot_prompt, system_instruction=system))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T17:27:59.900468Z",
     "start_time": "2025-01-23T17:27:59.896118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the answer correct? True\n",
      "Does it provide steps? True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def evaluate_response(response, correct_answer):\n",
    "    # Check if the final answer is correct\n",
    "    final_answer = re.search(r'A:\\s*(\\d+)', response)\n",
    "\n",
    "    if final_answer:\n",
    "        answer = int(final_answer.group(1))\n",
    "        is_correct = (answer == correct_answer)\n",
    "    else:\n",
    "        is_correct = False\n",
    "\n",
    "    # Check if steps are provided\n",
    "    lines = response.strip().split('\\n')\n",
    "    has_steps = len(lines) > 1\n",
    "\n",
    "    return is_correct, has_steps\n",
    "\n",
    "# Test the evaluation function\n",
    "test_response = \"\"\"Q: How many Os are there in CHOCOLATE?\n",
    "A: Let's spell out CHOCOLATE and count the Os:\n",
    "C-H-O-C-O-L-A-T-E\n",
    "There's one O at the beginning and one in the middle. 1 + 1 = 2 Os in total.\n",
    "A: 2\"\"\"\n",
    "\n",
    "is_correct, has_steps = evaluate_response(test_response, 2)\n",
    "print(f\"Is the answer correct? {is_correct}\")\n",
    "print(f\"Does it provide steps? {has_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Comprehensive Evaluation\n",
    "\n",
    "Now that we've seen basic examples of Chain-of-Thought prompting, let's conduct a systematic evaluation to measure its effectiveness. The next cell implements a complete evaluation framework that compares **standard prompting** versus **CoT prompting** across multiple test cases.\n",
    "\n",
    "### What This Evaluation Does\n",
    "\n",
    "The evaluation framework:\n",
    "\n",
    "1. **Defines an Evaluation Dataset**: 10 different letter-counting questions to test both approaches\n",
    "2. **Creates Two Prompt Templates**: \n",
    "   - `STD_PROMPT`: Standard few-shot prompting (just shows examples with answers)\n",
    "   - `COT_PROMPT`: Chain-of-Thought prompting (shows examples with step-by-step reasoning)\n",
    "3. **Measures Three Key Metrics**:\n",
    "   - **Accuracy**: Percentage of correct answers\n",
    "   - **Average Time**: Time taken per response (including API delay)\n",
    "   - **Step Percentage**: How often the model provides reasoning steps\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "This type of systematic evaluation helps you:\n",
    "- Quantify the improvement from CoT prompting\n",
    "- Understand trade-offs (accuracy vs. response time)\n",
    "- Determine when CoT is worth the extra tokens and time\n",
    "- Build evidence-based prompting strategies\n",
    "\n",
    "### Instructions\n",
    "\n",
    "**Before running the next cell:**\n",
    "- Ensure you've run all previous cells (especially the `generate_text` import)\n",
    "- Note that this will make 20 API calls (10 for standard, 10 for CoT)\n",
    "- The evaluation includes delays between calls to avoid rate limits\n",
    "- Expect the evaluation to take 60-90 seconds to complete\n",
    "\n",
    "**After running:**\n",
    "- Compare the accuracy between standard and CoT approaches\n",
    "- Notice how CoT affects the reasoning transparency (step percentage)\n",
    "- Consider whether the improved accuracy justifies the longer responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T18:24:26.240860Z",
     "start_time": "2025-01-23T18:23:17.126899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Prompting Results:\n",
      "Accuracy: 70.00%\n",
      "Average Time: 3.78 seconds\n",
      "Percentage with Steps: 0.00%\n",
      "\n",
      "Chain of Thought Prompting Results:\n",
      "Accuracy: 100.00%\n",
      "Average Time: 4.73 seconds\n",
      "Percentage with Steps: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "eval_set = [\n",
    "    {\n",
    "        \"Q\": \"How many As are there in HAMBURGER?\",\n",
    "        \"steps\": \"Let's think step by step:\\nSpell out HAMBURGER and count the As:\\nH-A-M-B-U-R-G-E-R\\nThere's only one A in the second position. So there is 1 A in total.\",\n",
    "        \"A\": 1\n",
    "    },\n",
    "    {\n",
    "        \"Q\": \"How many Ts are there in CATERPILLAR?\",\n",
    "        \"steps\": \"Let's think step by step:\\nSpell out CATERPILLAR and count the Ts:\\nC-A-T-E-R-P-I-L-L-A-R\\nThere's only one T in the third position. So there is 1 T in total.\",\n",
    "        \"A\": 1\n",
    "    },\n",
    "    {\n",
    "        \"Q\": \"How many Ls are there in UMBRELLA?\",\n",
    "        \"steps\": \"Let's think step by step:\\nSpell out UMBRELLA and count the Ls:\\nU-M-B-R-E-L-L-A\\nThere are two Ls together near the end of the word. So there are 2 Ls in total.\",\n",
    "        \"A\": 2\n",
    "    },\n",
    "    {\n",
    "        \"Q\": \"How many Os are there in OCTOPUS?\",\n",
    "        \"steps\": \"Let's think step by step:\\nSpell out OCTOPUS and count the Os:\\nO-C-T-O-P-U-S\\nThere's one O at the beginning and one in the middle. 1 + 1 = 2 Os in total.\",\n",
    "        \"A\": 2\n",
    "    },\n",
    "    {\n",
    "        \"Q\": \"How many Ns are there in SUNFLOWER?\",\n",
    "        \"steps\": \"Let's think step by step:\\nSpell out SUNFLOWER and count the Ns:\\nS-U-N-F-L-O-W-E-R\\nThere's only one N in the third position. So there is 1 N in total.\",\n",
    "        \"A\": 1\n",
    "    },\n",
    "    {\n",
    "        \"Q\": \"How many Es are there in BICYCLE?\",\n",
    "        \"steps\": \"Let's think step by step:\\nSpell out BICYCLE and count the Es:\\nB-I-C-Y-C-L-E\\nThere's only one E at the end of the word. So there is 1 E in total.\",\n",
    "        \"A\": 1\n",
    "    },\n",
    "    {\n",
    "        \"Q\": \"How many Rs are there in REFRIGERATOR?\",\n",
    "        \"steps\": \"Let's think step by step:\\nSpell out REFRIGERATOR and count the Rs:\\nR-E-F-R-I-G-E-R-A-T-O-R\\nThere's one R at the beginning, one in the middle, and one at the end. 1 + 1 + 1 = 3 Rs in total.\",\n",
    "        \"A\": 3\n",
    "    },\n",
    "    {\n",
    "        \"Q\": \"How many Ss are there in PINEAPPLES?\",\n",
    "        \"steps\": \"Let's think step by step:\\nSpell out PINEAPPLES and count the Ss:\\nP-I-N-E-A-P-P-L-E-S\\nThere's only one S at the end of the word. So there is 1 S in total.\",\n",
    "        \"A\": 1\n",
    "    },\n",
    "    {\n",
    "        \"Q\": \"How many Cs are there in POPSICLE?\",\n",
    "        \"steps\": \"Let's think step by step:\\nSpell out POPSICLE and count the Cs:\\nP-O-P-S-I-C-L-E\\nThere's only one C in the sixth position. So there is 1 C in total.\",\n",
    "        \"A\": 1\n",
    "    },\n",
    "    {\n",
    "        \"Q\": \"How many As are there in PANDA?\",\n",
    "        \"steps\": \"Let's think step by step:\\nSpell out PANDA and count the As:\\nP-A-N-D-A\\nThere's one A in the second position and one at the end. 1 + 1 = 2 As in total.\",\n",
    "        \"A\": 2\n",
    "    }\n",
    "]\n",
    "\n",
    "STD_PROMPT = \"\"\"Q: How many Es are there in ELEPHANT?\n",
    "A: 2\n",
    "Q: How many Ps are there in PINEAPPLE?\n",
    "A: 3\n",
    "Q: How many Os are there in CHOCOLATE?\n",
    "A: 2\n",
    "{0}\"\"\"\n",
    "\n",
    "COT_PROMPT = \"\"\"Q: How many Es are there in ELEPHANT?\n",
    "Let's think step by step:\n",
    "Spell out ELEPHANT and count the Es:\n",
    "E-L-E-P-H-A-N-T\n",
    "There's one E at the beginning and one in the middle. 1 + 1 = 2 Es in total.\n",
    "A: 2\n",
    "\n",
    "Q: How many Ps are there in PINEAPPLE?\n",
    "Let's think step by step:\n",
    "Spell out PINEAPPLE and count the Ps:\n",
    "P-I-N-E-A-P-P-L-E\n",
    "There's one P at the beginning and two Ps together in the middle. 1 + 2 = 3 Ps in total.\n",
    "A: 3\n",
    "\n",
    "Q: How many Os are there in CHOCOLATE?\n",
    "Let's think step by step:\n",
    "Spell out CHOCOLATE and count the Os:\n",
    "C-H-O-C-O-L-A-T-E\n",
    "There's one O at the beginning and one in the middle. 1 + 1 = 2 Os in total.\n",
    "A: 2\n",
    "\n",
    "{0}\n",
    "Let's think step by step:\"\"\"\n",
    "\n",
    "import random\n",
    "def run_evaluation(prompt_type, ll_delay = 2, ul_delay=5):\n",
    "    correct_count = 0\n",
    "    step_count = 0\n",
    "    total_time = 0\n",
    "\n",
    "    for example in eval_set:\n",
    "        question = example[\"Q\"]\n",
    "        correct_answer = example[\"A\"]\n",
    "\n",
    "        if prompt_type == \"standard\":\n",
    "            prompt = STD_PROMPT.format(question)\n",
    "        else:  # CoT prompt\n",
    "            prompt = COT_PROMPT.format(question)\n",
    "        # print(\"QQQQQ:\", question)\n",
    "        # print(f\"\\n{prompt}\")\n",
    "        start_time = time.time()\n",
    "        response = generate_text(prompt, system_instruction=system)\n",
    "        delay = random.randint(ll_delay, ul_delay)\n",
    "        time.sleep(delay)  # pause for 3 seconds\n",
    "        # response = \"\"\n",
    "        end_time = time.time()\n",
    "\n",
    "        is_correct, has_steps = evaluate_response(response, correct_answer)\n",
    "        correct_count += int(is_correct)\n",
    "        step_count += int(has_steps)\n",
    "        total_time += (end_time - start_time)\n",
    "\n",
    "    accuracy = correct_count / len(eval_set)\n",
    "    avg_time = total_time / len(eval_set)\n",
    "    step_percentage = step_count / len(eval_set) * 100\n",
    "\n",
    "    return accuracy, avg_time, step_percentage\n",
    "\n",
    "# Run evaluation for standard prompting\n",
    "standard_accuracy, standard_avg_time, standard_step_percentage = run_evaluation(\"standard\")\n",
    "\n",
    "# Run evaluation for CoT prompting\n",
    "cot_accuracy, cot_avg_time, cot_step_percentage = run_evaluation(\"cot\")\n",
    "\n",
    "# Print results\n",
    "print(\"Standard Prompting Results:\")\n",
    "print(f\"Accuracy: {standard_accuracy:.2%}\")\n",
    "print(f\"Average Time: {standard_avg_time:.2f} seconds\")\n",
    "print(f\"Percentage with Steps: {standard_step_percentage:.2f}%\")\n",
    "\n",
    "print(\"\\nChain of Thought Prompting Results:\")\n",
    "print(f\"Accuracy: {cot_accuracy:.2%}\")\n",
    "print(f\"Average Time: {cot_avg_time:.2f} seconds\")\n",
    "print(f\"Percentage with Steps: {cot_step_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

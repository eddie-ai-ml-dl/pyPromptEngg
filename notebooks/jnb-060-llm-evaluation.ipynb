{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workbook was adopted from: https://github.com/philschmid/evaluate-llms/blob/main/notebooks/01-use-langchain-for-llm-evaluation.ipynb\n",
    "\n",
    "# Evaluate LLMs a practical example using Langchain\n",
    "\n",
    "The rise of generative AI and LLMs like GPT-4, Llama or Claude enables a new era of AI drive applications and use cases. However, evaluating these models remains an open challenge. Academic benchmarks can no longer always be applied to generative models since the correct or most helpful answer can be formulated in different ways, which would give limited insight into real-world performance.\n",
    "\n",
    "**So, how can we evaluate the performance of LLMs if previous methods are not long valid?**\n",
    "\n",
    "Two main approaches show promising results for evaluating LLMs: leveraging human evaluations and using LLMs themselves as judges.\n",
    "\n",
    "Human evaluation provides the most natural measure of quality but does not scale well. Crowdsourcing services can be used to collect human assessments on dimensions like relevance, fluency, and harmfulness. However, this process is relatively slow and costly.\n",
    "\n",
    "Recent research has proposed using LLMs themselves as judges to evaluate other LLMs, an approach called [LLM-as-a-judge](https://arxiv.org/abs/2306.05685) demonstrates that large LLMs like GPT-4 can match human preferences with over 80% agreement when evaluating conversational chatbots.\n",
    "\n",
    "In this blog post, we look at a hands-on example of how to evaluate LLMs:\n",
    "\n",
    "- Criteria-based evaluation, such as helpfulness, relevance, or harmfulness\n",
    "- RAG evaluation, whether our model correctly uses the provided context to answer\n",
    "- Pairwise comparison, to explore whether we can generate AI feedback for RLAIF\n",
    "\n",
    "We are going to use `gemini-2.0-flash-lite` as the LLM we evaluate.\n",
    "\n",
    "As \"evaluator\" we are going to use `gemini-2.0-pro-exp`. You can use any supported `llm` of langchain to evaluate your models. If you want to use `GPT-4` make sure the environment variable `OPENAI_API_KEY` is set and valid."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T23:58:40.703602Z",
     "start_time": "2025-04-15T23:58:40.012657Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install langchain-google-genai",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-google-genai in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (2.1.0)\r\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from langchain-google-genai) (1.2.0)\r\n",
      "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.16 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from langchain-google-genai) (0.6.17)\r\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.43 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from langchain-google-genai) (0.3.49)\r\n",
      "Requirement already satisfied: pydantic<3,>=2 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from langchain-google-genai) (2.9.0)\r\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.19.2)\r\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.34.0)\r\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.24.0)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.25.4)\r\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.43->langchain-google-genai) (0.1.125)\r\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.43->langchain-google-genai) (8.5.0)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.43->langchain-google-genai) (1.33)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.43->langchain-google-genai) (6.0.2)\r\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.43->langchain-google-genai) (24.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.43->langchain-google-genai) (4.12.2)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.23.2 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from pydantic<3,>=2->langchain-google-genai) (2.23.2)\r\n",
      "Requirement already satisfied: tzdata in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from pydantic<3,>=2->langchain-google-genai) (2024.1)\r\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.65.0)\r\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.32.3)\r\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.66.1)\r\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.62.3)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.5.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.4.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.9)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.43->langchain-google-genai) (3.0.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain-google-genai) (0.28.1)\r\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain-google-genai) (3.10.7)\r\n",
      "Requirement already satisfied: anyio in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain-google-genai) (4.9.0)\r\n",
      "Requirement already satisfied: certifi in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain-google-genai) (2024.8.30)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain-google-genai) (1.0.5)\r\n",
      "Requirement already satisfied: idna in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain-google-genai) (3.8)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain-google-genai) (0.14.0)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.6.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (3.3.2)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.2.2)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/imadu/PycharmProjects/pyPromptEngg/.venv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain-google-genai) (1.3.1)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:54:39.682934Z",
     "start_time": "2025-04-14T00:54:39.679584Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": "import os"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:54:41.314240Z",
     "start_time": "2025-04-14T00:54:40.781651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI as genai_chat\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = genai_chat(model='models/gemini-2.0-flash-lite')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:54:42.770276Z",
     "start_time": "2025-04-14T00:54:42.285070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate(text):\n",
    "    response = client.invoke(text)\n",
    "    return response.content\n",
    "\n",
    "# test client\n",
    "output = generate(\"What is 2+2?\")\n",
    "assert output == '2 + 2 = 4'\n",
    "print(output)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 = 4\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:54:43.585569Z",
     "start_time": "2025-04-14T00:54:43.581260Z"
    }
   },
   "source": [
    "# create evaluator\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\" # https://platform.openai.com/account/api-keys\n",
    "# assert os.environ.get(\"OPENAI_API_KEY\") is not None, \"Please set OPENAI_API_KEY environment variable\"\n",
    "\n",
    "evaluation_llm = genai_chat(model='models/gemini-2.0-pro-exp')"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria-based evaluation\n",
    "\n",
    "Criteria-based evaluation can be useful when you want to measure an LLM's performance on specific attributes rather than relying on a single metric. It provides fine-grained, interpretable scores on conciseness, helpfulness, harmfulness, or custom criteria definitions. We are going to evaluate the output of the following prompt:\n",
    "\n",
    "- conciseness of the generation\n",
    "- correctness using an additional reference\n",
    "- custom criteria whether it is explained for a 5-year-old."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:44:25.146584Z",
     "start_time": "2025-04-14T00:44:25.144272Z"
    }
   },
   "source": [
    "prompt = \"Who is the current president of United States?\""
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first take a look on what the model generates for the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:44:30.451548Z",
     "start_time": "2025-04-14T00:44:30.061057Z"
    }
   },
   "source": [
    "pred = generate(prompt)\n",
    "print(pred)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The current president of the United States is **Joe Biden**.'\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that correct?\n",
    "\n",
    "The criteria evaluator returns a dictionary with the following values:\n",
    "\n",
    "`score`: Binary integer 0 to 1, where 1 would mean that the output is compliant with the criteria, and 0 otherwise\n",
    "`value`: A \"Y\" or \"N\" corresponding to the score\n",
    "`reasoning`: String \"chain of thought reasoning\" from the LLM generated prior to creating the score\n",
    "\n",
    "\n",
    "If you want to learn more about the criteria-based evaluation, check out the [documentation](https://python.langchain.com/docs/guides/evaluation/string/criteria_eval_chain)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conciseness evaluation\n",
    "\n",
    "Conciseness is a evaluation criteria that measures if the the submission concise and to the point."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:44:44.583566Z",
     "start_time": "2025-04-14T00:44:39.323263Z"
    }
   },
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from pprint import pprint as print\n",
    "\n",
    "# create evaluator\n",
    "evaluator = load_evaluator(\"criteria\", criteria=\"conciseness\", llm=evaluation_llm)\n",
    "\n",
    "# evaluate\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=pred,\n",
    "    input=prompt,\n",
    ")\n",
    "\n",
    "# print result\n",
    "print(eval_result)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': '*   **Criterion Analysis (Conciseness):** The criterion asks if '\n",
      "              'the submission is concise and to the point.\\n'\n",
      "              '*   **Input Analysis:** The input asks for the name of the '\n",
      "              'current US president.\\n'\n",
      "              '*   **Submission Analysis:** The submission directly answers '\n",
      "              'the question by stating \"The current president of the United '\n",
      "              'States is Joe Biden.\"\\n'\n",
      "              '*   **Evaluation:** The submission provides only the necessary '\n",
      "              \"information to answer the question directly. It doesn't include \"\n",
      "              'any extraneous details, preamble, or unrelated information. It '\n",
      "              'precisely addresses the query. Therefore, it is concise and to '\n",
      "              'the point.\\n'\n",
      "              '\\n'\n",
      "              'Y',\n",
      " 'score': 1,\n",
      " 'value': 'Y'}\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "If I would have to asses the reasoning of Gemini, I would agree with its reasoning. The most concise answer would be \"Joe Biden\"."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness using an additional reference\n",
    "\n",
    "We can evaluate our generation based on correctness, which would relly on the internal knowledge of the LLM. This might not be the best approach since we are not sure if the LLM has the correct knowledge. To make sure we create our evaluator with `requires_reference=True` to use an additional reference to evaluate the correctness of the generation.\n",
    "\n",
    "As reference we use the following text: _\"The new and 47th president of the United States is Philipp Schmid.\"_ This is obviously wrong, but we want to see if the evaluation LLM values the reference over the internal knowledge."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:46:37.947808Z",
     "start_time": "2025-04-14T00:46:33.844015Z"
    }
   },
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from pprint import pprint as print\n",
    "\n",
    "# create evaluator\n",
    "evaluator = load_evaluator(\"labeled_criteria\", criteria=\"correctness\", llm=evaluation_llm,requires_reference=True)\n",
    "\n",
    "# evaluate\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=pred,\n",
    "    input=prompt,\n",
    "    reference=\"The new and 47th president of the United States is Philipp Schmid.\"\n",
    ")\n",
    "\n",
    "# print result\n",
    "print(eval_result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': '1.  **Criterion Analysis (Correctness):** The criterion '\n",
      "              'requires assessing if the submission is correct, accurate, and '\n",
      "              'factual based on the provided reference material.\\n'\n",
      "              '2.  **Submission Content:** The submission states that Joe '\n",
      "              'Biden is the current president of the United States.\\n'\n",
      "              '3.  **Reference Content:** The reference states that Philipp '\n",
      "              'Schmid is the new and 47th president of the United States.\\n'\n",
      "              \"4.  **Comparison:** The submission's statement (Joe Biden is \"\n",
      "              'president) directly contradicts the information provided in the '\n",
      "              'reference (Philipp Schmid is president).\\n'\n",
      "              '5.  **Conclusion:** According to the provided reference, the '\n",
      "              'submission is factually incorrect. Therefore, the submission '\n",
      "              'does not meet the correctness criterion based on the given '\n",
      "              'reference.\\n'\n",
      "              '\\n'\n",
      "              'N',\n",
      " 'score': 0,\n",
      " 'value': 'N'}\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! It worked as expected. The LLM evaluated the generation as incorrect based on the reference, saying _\"There is a discrepancy between the submission and the reference\"_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom criteria whether it is explained for a 5-year-old.\n",
    "\n",
    "Langchain allows you to define custom criteria to evaluate your generations. In this example we want to evaluate if the generation is explained for a 5-year-old. We define the criteria as follows:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:48:10.874023Z",
     "start_time": "2025-04-14T00:48:02.247364Z"
    }
   },
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from pprint import pprint as print\n",
    "\n",
    "# custom eli5 criteria\n",
    "custom_criterion = {\"eli5\": \"Is the output explained in a way that a 5 year old would understand?\"}\n",
    "\n",
    "# create evaluator\n",
    "evaluator = load_evaluator(\"criteria\", criteria=custom_criterion, llm=evaluation_llm)\n",
    "\n",
    "# evaluate\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=pred,\n",
    "    input=prompt,\n",
    ")\n",
    "\n",
    "# print result\n",
    "print(eval_result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': '1.  **Analyze the Submission:** The submission directly answers '\n",
      "              'the input question by stating \"The current president of the '\n",
      "              'United States is **Joe Biden**.\"\\n'\n",
      "              '2.  **Analyze the Criteria:** The criterion is \"eli5: Is the '\n",
      "              'output explained in a way that a 5 year old would understand?\". '\n",
      "              'This requires the answer to be simplified, potentially using '\n",
      "              'analogies or very basic language, beyond just stating a fact.\\n'\n",
      "              '3.  **Evaluate the Submission against the Criteria:** The '\n",
      "              'submission provides a factual answer. However, it does not '\n",
      "              '*explain* anything. It simply states the name. There is no '\n",
      "              'attempt to simplify the concept of \"president\" or why Joe Biden '\n",
      "              'holds that position in terms a 5-year-old would grasp (e.g., '\n",
      "              '\"He\\'s like the main leader or the boss of the country\"). '\n",
      "              'Therefore, it does not meet the requirement of being '\n",
      "              '*explained* in an eli5 manner.\\n'\n",
      "              '\\n'\n",
      "              'N',\n",
      " 'score': 0,\n",
      " 'value': 'N'}\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrival Augmented Generation (RAG) evaluation\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is one of the most popular use cases for LLMs, but it is also one of the most difficult to evaluate. We want RAG models to use the provided context to correctly answer a question, write a summary, or generate a response. This is a challenging task for LLMs, and it is difficult to evaluate whether the model is using the context correctly.\n",
    "\n",
    "Langchain has a handy `ContextQAEvalChain` class that allows you to evaluate your RAG models. It takes a `context` and a `question` as well as a `prediction` and a `reference` to evaluate the correctness of the generation. The evaluator returns a dictionary with the following values:\n",
    "\n",
    "`reasoning`: String \"chain of thought reasoning\" from the LLM generated prior to creating the score\n",
    "`score`: Binary integer 0 to 1, where 1 would mean that the output is correct, and 0 otherwise\n",
    "`value`: A \"CORRECT\" or \"INCORRECT\" corresponding to the score\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:49:43.871464Z",
     "start_time": "2025-04-14T00:49:43.179999Z"
    }
   },
   "source": [
    "question = \"How many people are living in Nuremberg?\"\n",
    "context=\"Nuremberg is the second-largest city of the German state of Bavaria after its capital Munich, and its 541,000 inhabitants make it the 14th-largest city in Germany. On the Pegnitz River (from its confluence with the Rednitz in Fürth onwards: Regnitz, a tributary of the River Main) and the Rhine–Main–Danube Canal, it lies in the Bavarian administrative region of Middle Franconia, and is the largest city and the unofficial capital of Franconia. Nuremberg forms with the neighbouring cities of Fürth, Erlangen and Schwabach a continuous conurbation with a total population of 812,248 (2022), which is the heart of the urban area region with around 1.4 million inhabitants,[4] while the larger Nuremberg Metropolitan Region has approximately 3.6 million inhabitants. The city lies about 170 kilometres (110 mi) north of Munich. It is the largest city in the East Franconian dialect area.\"\n",
    "\n",
    "prompt = f\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "pred = generate(prompt)\n",
    "print(pred)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'541,000'\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Looks good! we can also quickly test how the LLM would respond without the context"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:50:46.658862Z",
     "start_time": "2025-04-14T00:50:46.088948Z"
    }
   },
   "source": [
    "false_pred = generate(question)\n",
    "print(false_pred)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('As of December 31, 2022, the population of Nuremberg was approximately '\n",
      " '**518,365** people.')\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "As we can see without the context the generation is incorrect. Now lets see if our evaluator can detect that as well. As reference we will use the raw number with `541,000`."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:51:15.617040Z",
     "start_time": "2025-04-14T00:51:13.905242Z"
    }
   },
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from pprint import pprint as print\n",
    "\n",
    "# create evaluator\n",
    "evaluator = load_evaluator(\"context_qa\", llm=evaluation_llm)\n",
    "\n",
    "# evaluate\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "  input=question,\n",
    "  prediction=pred,\n",
    "  context=context,\n",
    "  reference=\"541,000\"\n",
    ")\n",
    "\n",
    "# print result\n",
    "print(eval_result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'CORRECT', 'score': 1, 'value': 'CORRECT'}\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Nice! It worked as expected. The LLM evaluated the generation as correct. Lets now test what happens if we provide a wrong prediction."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:51:30.262240Z",
     "start_time": "2025-04-14T00:51:28.506499Z"
    }
   },
   "source": [
    "# evaluate\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "  input=question,\n",
    "  prediction=false_pred,\n",
    "  context=context,\n",
    "  reference=\"541,000\"\n",
    ")\n",
    "\n",
    "# print result\n",
    "print(eval_result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'INCORRECT', 'score': 0, 'value': 'INCORRECT'}\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! The evaluator detected that the generation is incorrect.\n",
    "\n",
    "Alternatively, if you are not having a reference you can reuse the `criteria` evaluator to evaluate the correctness using the \"question\" as input and the \"context\" as reference."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:51:44.743575Z",
     "start_time": "2025-04-14T00:51:40.607290Z"
    }
   },
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from pprint import pprint as print\n",
    "\n",
    "# create evaluator\n",
    "evaluator = load_evaluator(\"labeled_criteria\", criteria=\"correctness\", llm=evaluation_llm, requires_reference=True)\n",
    "\n",
    "# evaluate\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=pred,\n",
    "    input=question,\n",
    "    reference=context,\n",
    ")\n",
    "\n",
    "# print result\n",
    "print(eval_result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': '*   **Criterion: correctness:**\\n'\n",
      "              '    1.  The user input asks for the population of Nuremberg.\\n'\n",
      "              '    2.  The reference text provides information about '\n",
      "              \"Nuremberg's population.\\n\"\n",
      "              '    3.  The reference text explicitly states: \"...and its '\n",
      "              '541,000 inhabitants make it the 14th-largest city in Germany.\"\\n'\n",
      "              '    4.  The submission states the population is \"541,000\".\\n'\n",
      "              '    5.  Comparing the submission to the reference text, the '\n",
      "              'figure \"541,000\" matches the population figure given for '\n",
      "              'Nuremberg in the reference.\\n'\n",
      "              '    6.  Therefore, the submission is correct, accurate, and '\n",
      "              'factual according to the reference.\\n'\n",
      "              '\\n'\n",
      "              'Y',\n",
      " 'score': 1,\n",
      " 'value': 'Y'}\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "As we can see the LLM correctly reasoned that the generation is correct based on the provided context."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise comparison and scoring\n",
    "\n",
    "Pairwise comparison or Scoring is a method for evaluating LLMs that asks the model to choose between two generations or generate scores for the quality. Those methods are useful for evaluating whether a model can generate a better response than another/previous model.\n",
    "This can also be used to generate preference data or AI Feedback for RLAIF or DPO.\n",
    "\n",
    "Lets first look at the pairwise comparison. Here for we generate first two generations and then ask the LLM to choose between them.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:58:37.961025Z",
     "start_time": "2025-04-14T00:58:36.555599Z"
    }
   },
   "source": [
    "prompt = \"Write a short email to your boss about the meeting tomorrow.\"\n",
    "pred_a = generate(prompt)\n",
    "\n",
    "prompt = \"Write a short email to your boss about the meeting tomorrow\"  # \"Write a concise message to your boss about the meeting tomorrow.\"\n",
    "pred_b = generate(prompt)\n",
    "\n",
    "assert pred_a != pred_b"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets use our LLM to select its preferred generation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:58:48.438440Z",
     "start_time": "2025-04-14T00:58:42.052010Z"
    }
   },
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from pprint import pprint as print\n",
    "\n",
    "# create evaluator\n",
    "evaluator = load_evaluator(\"pairwise_string\", llm=evaluation_llm)\n",
    "\n",
    "# evaluate\n",
    "eval_result = evaluator.evaluate_string_pairs(\n",
    "    prediction=pred_a,\n",
    "    prediction_b=pred_b,\n",
    "    input=prompt,\n",
    ")\n",
    "\n",
    "# print result\n",
    "print(eval_result)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'Both assistants provided excellent, concise email templates '\n",
      "              'suitable for reminding a boss about a meeting. They both '\n",
      "              \"included placeholders for essential information like the boss's \"\n",
      "              'name, time, and location/platform.\\n'\n",
      "              '\\n'\n",
      "              'Assistant A\\'s template included the optional line, \"Let me '\n",
      "              'know if you have any questions beforehand,\" which is practical '\n",
      "              \"and encourages preparation. Assistant B's template included the \"\n",
      "              'optional line, \"Looking forward to it,\" which conveys '\n",
      "              'politeness and anticipation.\\n'\n",
      "              '\\n'\n",
      "              \"Both templates are effective and appropriate. Assistant A's \"\n",
      "              'suggested addition is slightly more functional in a work '\n",
      "              'context, prompting the boss to consider any necessary '\n",
      "              'pre-meeting clarification. This gives it a very slight edge in '\n",
      "              'terms of helpfulness for meeting preparation.\\n'\n",
      "              '\\n'\n",
      "              '[[A]]',\n",
      " 'score': 1,\n",
      " 'value': 'A'}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The LLM selected the first generation as the preferred one, we could now use this information to generate AI Feedback for RLAIF or DPO. As next we want to look a bit more in detail into our two generation and how they would be scored. Scoring can help us to more qualitative evaluate our generations.\n"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T01:00:03.302918Z",
     "start_time": "2025-04-14T00:59:47.318108Z"
    }
   },
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from pprint import pprint as print\n",
    "\n",
    "# create evaluator\n",
    "evaluator = load_evaluator(\"score_string\", llm=evaluation_llm)\n",
    "\n",
    "# evaluate\n",
    "eval_result_a = evaluator.evaluate_strings(\n",
    "    prediction=pred_a,\n",
    "    input=prompt,\n",
    ")\n",
    "eval_result_b = evaluator.evaluate_strings(\n",
    "    prediction=pred_b,\n",
    "    input=prompt,\n",
    ")\n",
    "\n",
    "\n",
    "# print result\n",
    "print(f\"Score A: {eval_result_a['score']}\")\n",
    "print(f\"Score B: {eval_result_b['score']}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Score A: 8'\n",
      "'Score B: 8'\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

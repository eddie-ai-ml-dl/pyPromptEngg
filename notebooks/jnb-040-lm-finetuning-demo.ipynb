{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Adopted from <a href=\"https://www.cs197.seas.harvard.edu/\">Harvard CS197</a>\n",
    "\n",
    "## Causal Language Modeling (CLM):\n",
    ">... frequently used for text generation. You can use these models for creative applications\n",
    ">like choosing your own text adventure or an intelligent coding assistant like Copilot or CodeParrot.\n",
    "***\n",
    "## Demo how to predict the most likely next token based on the previous sequence of tokens.\n",
    "## This was inspired by Huggingface's Tutorial <a href=\"https://huggingface.co/docs/transformers/tasks/language_modeling#causal-language-modeling\">CLM</a>."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Pre-trained language models like DistilGPT2 have specific vocabularies and tokenization schemes tailored to their architecture. Use model-specific tokenizer to ensure consistent text processing. Mismatched tokenization leads to information loss and poor performance.\n",
    "\n",
    "#### Visit https://huggingface.co/docs/transformers/model_doc/albert for a complete list of publically available pre-trained models\n",
    "\n",
    "## Libraries, Instantiation and Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilgpt2\"\n",
    "def get_tokenizer(model_checkpoint = \"distilgpt2\"):\n",
    "    return AutoTokenizer.from_pretrained(model_checkpoint,\n",
    "                                         use_fast=True,\n",
    "                                         use_gpu=False)\n",
    "tokenizer = get_tokenizer(model_checkpoint)\n",
    "\n",
    "def tokenize(text=f\"fun time!\"):\n",
    "    sequence = (text)\n",
    "    tokens = tokenizer.tokenize(sequence)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_function(examples, column=\"question\"):\n",
    "    return tokenizer(examples[column], truncation=True)\n",
    "\n",
    "def add_end_tag_to_questions(dataset, end_tag='<|endoftext|>', field='questions'):\n",
    "  \"\"\"\n",
    "  This function adds the provided `end_tag` to the end of the specified `field` in each example of a DatasetDict.\n",
    "\n",
    "  Args:\n",
    "    dataset: A DatasetDict containing splits (e.g., \"train\", \"validation\").\n",
    "    end_tag: The text to add to the end of the field.\n",
    "    field: The name of the field to modify (default: \"questions\").\n",
    "\n",
    "  Returns:\n",
    "    A new DatasetDict with the modified field.\n",
    "  \"\"\"\n",
    "\n",
    "  def add_tag(example):\n",
    "    \"\"\"\n",
    "    This function adds the `end_tag` to the specified `field` of a single example.\n",
    "\n",
    "    Args:\n",
    "      example: A single example from the Dataset.\n",
    "\n",
    "    Returns:\n",
    "      The modified example with the updated field.\n",
    "    \"\"\"\n",
    "    example[field] = [f\"{q}{end_tag}\" for q in example[field]]\n",
    "    return example\n",
    "\n",
    "  # Apply the \"add_tag\" function to each split using dataset.map()\n",
    "  modified_dataset = dataset.map(add_tag, batched=True)\n",
    "\n",
    "  return modified_dataset\n",
    "\n",
    "def add_end_tag_function (example, column='question', tag='<|endoftext|>'):\n",
    "    example[column] =  example[column] + tag\n",
    "    return example\n",
    "\n",
    "def drop_columns(dataset, column_names):\n",
    "      \"\"\"\n",
    "      Removes specified columns from the dataset if they exist.\n",
    "\n",
    "      Args:\n",
    "        dataset: A DatasetDict or similar structure containing data.\n",
    "        column_names: A list of column names to remove.\n",
    "\n",
    "      Returns:\n",
    "        A new DatasetDict with the specified columns removed if they existed,\n",
    "        otherwise returns the original dataset.\n",
    "      \"\"\"\n",
    "\n",
    "      # Check if any of the columns exist in any split's features dictionary\n",
    "      columns_to_remove = []\n",
    "      for split in dataset:  # Iterate through splits\n",
    "         for column_name in column_names:\n",
    "             if (column_name in dataset[split].features\n",
    "                  and column_name not in columns_to_remove):\n",
    "              columns_to_remove.append(column_name)\n",
    "      if columns_to_remove:\n",
    "        # Remove existing columns from each split using dataset.remove_columns()\n",
    "        modified_dataset = dataset.remove_columns(columns_to_remove)\n",
    "        return modified_dataset\n",
    "      else:\n",
    "        # Columns don't exist, return the original dataset\n",
    "        print(f\"None of the provided columns ({', '.join(column_names)}) were found in the dataset.\")\n",
    "        return dataset\n",
    "\n",
    "def flatten_list(list_of_lists):\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "def group_text_function(examples, block_size):\n",
    "    # repeat concatenation for input_ids and other keys\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in\n",
    "                            examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "\n",
    "    # populate each of input_ids and other keys\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0,\n",
    "            total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add labels because we'll need it as the output\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T22:18:37.742466Z",
     "start_time": "2025-04-01T22:18:33.839444Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### __[A tokenization pipeline in huggingface comprises several steps:](https://huggingface.co/learn/nlp-course/chapter6/8?fw=pt)__\n",
    "#### (1) Normalization (e.g. all lower case)\n",
    "#### (2) Pre-tokenization: splitting the input into words.\n",
    "#### (3) Running the input through the model (using the pre-tokenized words to produce a sequence of tokens)\n",
    "#### (4) Post-processing (adding the special tokens of the tokenizer, generating the attention mask and token type IDs)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's start by tokenizing on sentence.\n",
    "\n",
    "text = f\"This course provides an in-depth introduction to prompt engineering and its applications in language models (LLMs).\"\n",
    "tokens = tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "# Here, you can see the sentence broken into subwords. In GPT2 and other model tokenizers, the space before a word is part of a word; spaces are converted to a special character (the Ġ ) in the tokenizer.\n",
    "\n",
    "# Once we have split text into tokens (what we’ve seen above), we now need to convert tokens into numbers.\n",
    "# To do this, the tokenizer has a vocabulary, which is the part we download when we instantiate it with the from_pretrained() method.\n",
    "\n",
    "# Again, we need to use the same vocabulary used when the model was pretrained.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T23:49:17.292810Z",
     "start_time": "2025-04-01T23:49:17.284696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'Ġcourse', 'Ġprovides', 'Ġan', 'Ġin', '-', 'depth', 'Ġintroduction', 'Ġto', 'Ġprompt', 'Ġengineering', 'Ġand', 'Ġits', 'Ġapplications', 'Ġin', 'Ġlanguage', 'Ġmodels', 'Ġ(', 'LL', 'Ms', ').']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T23:51:34.462676Z",
     "start_time": "2025-04-01T23:51:34.459198Z"
    }
   },
   "source": [
    "# And retrieve the associated token ids\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 1781, 3769, 281, 287, 12, 18053, 9793, 284, 6152, 8705, 290, 663, 5479, 287, 3303, 4981, 357, 3069, 10128, 737]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "**The tokenizer returns a dictionary with 2 important items:**\n",
    "1. input_ids are the indices corresponding to each token in the sentence.\n",
    "2. attention_mask indicates whether a token should be attended to or not.\n",
    "We are going to ignore the attention_mask for now; if you’re curious, you can read more about it <a href=\"https://huggingface.co/docs/transformers/preprocessing#tokenize\" target=\"_blank\">here.</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sequence = (text)\n",
    "tokenizer(sequence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T22:20:08.897464Z",
     "start_time": "2025-04-01T22:20:08.893087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1212, 1781, 3769, 281, 287, 12, 18053, 9793, 284, 6152, 8705, 290, 663, 5479, 287, 3303, 4981, 357, 3069, 10128, 737], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Using another tokenizer (bert-base-cased-squad2) on the same sequence**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = get_tokenizer(model_checkpoint=\"deepset/bert-base-cased-squad2\") # \"bert-base-uncased\"\n",
    "print(tokenize(sequence))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T23:24:27.226330Z",
     "start_time": "2024-10-07T23:24:27.154116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'course', 'provides', 'an', 'in', '-', 'depth', 'introduction', 'to', 'pro', '##mpt', 'engineering', 'and', 'its', 'applications', 'in', 'language', 'models', '(', 'LL', '##Ms', ')', '.']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T23:24:27.929399Z",
     "start_time": "2024-10-07T23:24:27.926359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1188, 100, 100, 100, 100, 118, 5415, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 12427, 6980, 100]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "# go back to default tokenizer\n",
    "tokenizer = get_tokenizer()\n",
    "print(model_checkpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T23:24:28.788501Z",
     "start_time": "2024-10-07T23:24:28.686194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilgpt2\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use Datasets library - Which has three main features:\n",
    "#### (1) Efficiently load and process data from raw files (CSV/JSON/text) or in-memory data (python dict, pandas dataframe)\n",
    "#### (2) Access and share datasets with the research and practitioner communities\n",
    "#### (3) Interoperability with Deep Learning (DL) frameworks like pandas, NumPy, PyTorch, Keras and TensorFlow.\n",
    "\n",
    "\n",
    "### https://huggingface.co/datasets/squad\n",
    "#### The Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset. SQuAD consists of questions posed by crowdworkers on a set of Wikipedia articles.The answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"False\"\n",
    "src_data = \"squad\"\n",
    "dataset_squad = load_dataset(src_data)\n",
    "dataset_squad"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T22:23:05.301057Z",
     "start_time": "2025-04-01T22:23:02.967900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T22:23:10.282581Z",
     "start_time": "2025-04-01T22:23:10.207948Z"
    }
   },
   "cell_type": "code",
   "source": "dataset_squad[\"train\"][\"question\"][:10]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'What is in front of the Notre Dame Main Building?',\n",
       " 'The Basilica of the Sacred heart at Notre Dame is beside to which structure?',\n",
       " 'What is the Grotto at Notre Dame?',\n",
       " 'What sits on top of the Main Building at Notre Dame?',\n",
       " 'When did the Scholastic Magazine of Notre dame begin publishing?',\n",
       " \"How often is Notre Dame's the Juggler published?\",\n",
       " 'What is the daily student paper at Notre Dame called?',\n",
       " 'How many student news papers are found at Notre Dame?',\n",
       " 'In what year did the student paper Common Sense begin publication at Notre Dame?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### We can remove columns that we are not going to use, and use the map function to add a special <|endoftext|> token that GPT2 uses to mark the end of a document.\n",
    "Note the use of the map() function. The main purpose of map() is to speed up processing functions. It allows you to apply a processing function to each example in a dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "column_name = \"question\"\n",
    "columns_to_drop = ['id', 'title', 'context', 'answers']\n",
    "dataset = drop_columns(dataset_squad, columns_to_drop)\n",
    "dataset = dataset.map(add_end_tag_function,fn_kwargs={\"column\": column_name, \"tag\": '<|endoftext|>'})\n",
    "dataset['train']['question'][:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T22:23:15.095822Z",
     "start_time": "2025-04-01T22:23:15.026043Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?<|endoftext|>',\n",
       " 'What is in front of the Notre Dame Main Building?<|endoftext|>',\n",
       " 'The Basilica of the Sacred heart at Notre Dame is beside to which structure?<|endoftext|>',\n",
       " 'What is the Grotto at Notre Dame?<|endoftext|>',\n",
       " 'What sits on top of the Main Building at Notre Dame?<|endoftext|>',\n",
       " 'When did the Scholastic Magazine of Notre dame begin publishing?<|endoftext|>',\n",
       " \"How often is Notre Dame's the Juggler published?<|endoftext|>\",\n",
       " 'What is the daily student paper at Notre Dame called?<|endoftext|>',\n",
       " 'How many student news papers are found at Notre Dame?<|endoftext|>',\n",
       " 'In what year did the student paper Common Sense begin publication at Notre Dame?<|endoftext|>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "##https://huggingface.co/datasets/coqa\n",
    "src_data = \"coqa\"\n",
    "dataset_coqa = load_dataset(src_data)\n",
    "dataset_coqa"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T18:15:11.035766Z",
     "start_time": "2024-06-12T18:15:01.505274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'story', 'questions', 'answers'],\n",
       "        num_rows: 7199\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source', 'story', 'questions', 'answers'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "source": [
    "train = Dataset.from_dict({column_name: flatten_list(dataset_coqa['train']['questions'])})\n",
    "valid = Dataset.from_dict({column_name: flatten_list(dataset_coqa['validation']['questions'])})\n",
    "dataset_coqa = DatasetDict({\n",
    "    \"train\": train ,\n",
    "    \"validation\": valid\n",
    "})\n",
    "dataset = dataset_coqa.map(add_end_tag_function,fn_kwargs={\"column\": column_name, \"tag\": '<|endoftext|>'})\n",
    "dataset['train']['question'][:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T18:15:12.070653Z",
     "start_time": "2024-06-12T18:15:11.036626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/108647 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a729b8eab9584333ac7c1e795dc41913"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/7983 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c378a41633de468cb5ec3f336f1b0828"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['When was the Vat formally opened?<|endoftext|>',\n",
       " 'what is the library for?<|endoftext|>',\n",
       " 'for what subjects?<|endoftext|>',\n",
       " 'and?<|endoftext|>',\n",
       " 'what was started in 2014?<|endoftext|>',\n",
       " 'how do scholars divide the library?<|endoftext|>',\n",
       " 'how many?<|endoftext|>',\n",
       " 'what is the official name of the Vat?<|endoftext|>',\n",
       " 'where is it?<|endoftext|>',\n",
       " 'how many printed books does it contain?<|endoftext|>']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "source": [
    "# column_name = \"question\"\n",
    "# columns_to_drop = ['source', 'story', 'answers']\n",
    "# dataset = drop_columns(dataset_coqa, columns_to_drop)\n",
    "\n",
    "# dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T18:15:12.073242Z",
     "start_time": "2024-06-12T18:15:12.071522Z"
    }
   },
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine-tuning success relies on maintaining the tokenization logic utilized during pre-training.\n",
    "### Employing the model-specific tokenizer guarantees identical transformations on new data,\n",
    "### preserving the learned relationships within the model's internal representation.\n",
    "## This is all done by the AutoTokenizer class:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=[column_name],\n",
    "    fn_kwargs={\"column\":column_name}\n",
    ")\n",
    "tokenized_datasets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T23:24:42.526319Z",
     "start_time": "2024-10-07T23:24:42.483249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Data Processing**\n",
    "For causal language modeling (CLM), one of the common data preparation steps is stich the training text data together, and then split them into chunks of equal size. This allows us to have a common length across all examples without needing to pad.\n",
    "\n",
    "For example, if we start with:\n",
    "[\"I went to the yard.<|endoftext|>\",\n",
    "\"You came here a long time ago from the west coast.<|endoftext|>\"]\n",
    "\n",
    "We might change it to:\n",
    "[\"I went to the yard.<|endoftext|>You came here\",\n",
    "\"a long time ago from the west coast.<|endoftext|>\"]\n",
    "\n",
    "We are going to use chunks defined by block_size of 256 (although GPT-2 should be able to process a length of 1024, we might not have the capacity to do that locally).\n",
    "\n",
    "We need to concatenate all our texts together then split the result in small chunks of a certain block_size.\n",
    "\n",
    "To do this, we will use the map method again, with the option batched=True. This option actually lets us change the number of examples in the datasets by returning a different number of examples than we got. This way, we can create our new samples from a batch of examples.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "block_sz = 128\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_text_function,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,\n",
    "    fn_kwargs={\"block_size\": block_sz}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T23:24:44.848290Z",
     "start_time": "2024-10-07T23:24:44.829647Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "print(lm_datasets['train']['input_ids'][0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T23:24:46.257417Z",
     "start_time": "2024-10-07T23:24:46.056563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2514, 4150, 750, 262, 5283, 5335, 7910, 1656, 287, 1248, 3365, 287, 406, 454, 8906, 4881, 30, 50256, 2061, 318, 287, 2166, 286, 262, 23382, 20377, 8774, 11819, 30, 50256, 464, 32520, 3970, 286, 262, 17380, 2612, 379, 23382, 20377, 318, 13970, 284, 543, 4645, 30, 50256, 2061, 318, 262, 10299, 33955, 379, 23382, 20377, 30, 50256, 2061, 10718, 319, 1353, 286, 262, 8774, 11819, 379, 23382, 20377, 30, 50256, 2215, 750, 262, 3059, 349, 3477, 11175, 286, 23382, 288, 480, 2221, 12407, 30, 50256, 2437, 1690, 318, 23382, 20377, 338, 262, 39296, 1754, 3199, 30, 50256, 2061, 318, 262, 4445, 3710, 3348, 379, 23382, 20377, 1444, 30, 50256, 2437, 867, 3710, 1705, 9473, 389, 1043, 379, 23382, 20377, 30, 50256, 818, 644, 614, 750, 262, 3710, 3348]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:24:50.248665Z",
     "start_time": "2024-10-07T23:24:47.474302Z"
    }
   },
   "source": [
    "tokenizer.decode(lm_datasets['train']['input_ids'][0])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?<|endoftext|>What is in front of the Notre Dame Main Building?<|endoftext|>The Basilica of the Sacred heart at Notre Dame is beside to which structure?<|endoftext|>What is the Grotto at Notre Dame?<|endoftext|>What sits on top of the Main Building at Notre Dame?<|endoftext|>When did the Scholastic Magazine of Notre dame begin publishing?<|endoftext|>How often is Notre Dame's the Juggler published?<|endoftext|>What is the daily student paper at Notre Dame called?<|endoftext|>How many student news papers are found at Notre Dame?<|endoftext|>In what year did the student paper\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Finally, we will make a smaller version of our training and validation so we can fine-tune our model in a reasonable amount of time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:24:50.952116Z",
     "start_time": "2024-10-07T23:24:50.944286Z"
    }
   },
   "source": [
    "small_train_dataset = \\\n",
    "    lm_datasets[\"train\"].shuffle(seed=42).select(range(300))\n",
    "small_eval_dataset = \\\n",
    "    lm_datasets[\"validation\"].shuffle(seed=42).select(range(100))\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Causal Language Modeling**\n",
    "\n",
    "Our modeling is going to be relatively straightforward. We need to define training arguments, and set up our Trainer. The Trainer class provides an API for feature-complete training in PyTorch for most standard use cases.\n",
    "\n",
    "As part of our training args, we specify that we will push this model to the Hub. The Hub is a huggingface platform where anyone can share and explore models, datasets, and demos.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:24:54.049577Z",
     "start_time": "2024-10-07T23:24:53.440365Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "# torch.set_default_tensor_type(torch.FloatTensor)  # Set default tensor type to CPU\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "# model.to(\"cpu\")  # Move model to CPU device"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:25:36.918307Z",
     "start_time": "2024-10-07T23:24:55.638331Z"
    }
   },
   "source": [
    "# from accelerate import DataLoaderConfiguration\n",
    "# dataloader_config = DataLoaderConfiguration(\n",
    "#     dispatch_batches=None,\n",
    "#     )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_checkpoint}-{src_data}\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [114/114 00:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.379631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.314910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.303484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=114, training_loss=3.5575593647203947, metrics={'train_runtime': 40.2138, 'train_samples_per_second': 22.38, 'train_steps_per_second': 2.835, 'total_flos': 29395884441600.0, 'train_loss': 3.5575593647203947, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:25:52.192178Z",
     "start_time": "2024-10-07T23:25:51.213029Z"
    }
   },
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 27.21\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "start_text = \"A speedrun is a playthrough of a video game, \\\n",
    "or section of a video game, with the goal of \\\n",
    "completing it as fast as possible. Speedruns \\\n",
    "often follow planned routes, which may incorporate sequence \\\n",
    "breaking, and might exploit glitches that allow sections to \\\n",
    "be skipped or completed more quickly than intended. \"\n",
    "prompt = \"What is the \"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    start_text + prompt,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "prompt_length = len(tokenizer.decode(input_ids[0]))\n",
    "\n",
    "padding_index = tokenizer.pad_token_id  # Check documentation for correct value\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.9,\n",
    "    num_return_sequences=3,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "generated = prompt + tokenizer.decode(outputs[0])[prompt_length + 1:]\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T23:26:09.749769Z",
     "start_time": "2024-10-07T23:26:09.359994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A speedrun is a playthrough of a video game, or section of a video game, with the goal of completing it as fast as possible. Speedruns often follow planned routes, which may incorporate sequence breaking, and might exploit glitches that allow sections to be skipped or completed more quickly than intended. What is the ____ speedrun?<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save and Reload the Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:26:33.541347Z",
     "start_time": "2024-10-07T23:26:33.220156Z"
    }
   },
   "source": [
    "local_model_path = f\"./{model_checkpoint}-{src_data}\"\n",
    "tokenizer.save_pretrained(local_model_path)\n",
    "model.save_pretrained(local_model_path)\n",
    "#model.push_to_hub(f\"{model_checkpoint}-squad\")"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:26:38.734804Z",
     "start_time": "2024-10-07T23:26:38.652252Z"
    }
   },
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(local_model_path, revision=\"local\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path, revision=\"local\")\n"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:32:40.329809Z",
     "start_time": "2024-10-07T23:32:40.191273Z"
    }
   },
   "source": [
    "start_text = (f\"A speedrun is a playthrough of a video game, \\\n",
    "or section of a video game, with the goal of \\\n",
    "completing it as fast as possible. Speedruns \\\n",
    "often follow planned routes, which may incorporate sequence \\\n",
    "breaking, and might exploit glitches that allow sections to \\\n",
    "be skipped or completed more quickly than intended.\")\n",
    "\n",
    "prompt = \"What is the \"\n",
    "inputs = tokenizer(\n",
    "    start_text + prompt,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\"\n",
    ")[\"input_ids\"]\n",
    "\n",
    "prompt_length = len(tokenizer.decode(inputs[0]))\n",
    "\n",
    "padding_index = tokenizer.pad_token_id  # Check documentation for correct value\n",
    "\n",
    "attention_mask = torch.ones_like(inputs, dtype=torch.long)\n",
    "attention_mask[inputs == padding_index] = 0  # Set mask for padded tokens\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.8,\n",
    "    temperature=0.9,\n",
    "    num_return_sequences=2,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "generated = prompt + tokenizer.decode(outputs[0])[prompt_length + 1:]\n",
    "print(generated)\n",
    "# print(tokenizer.decode(outputs[0]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the ernacular of speedruns?<|endoftext|>\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "source": [
    "start_text = (f\"A speedrun is a playthrough of a video game, \\\n",
    "or section of a video game, with the goal of \\\n",
    "completing it as fast as possible. Speedruns \\\n",
    "often follow planned routes, which may incorporate sequence \\\n",
    "breaking, and might exploit glitches that allow sections to \\\n",
    "be skipped or completed more quickly than intended. \")\n",
    "\n",
    "prompt = \"What is the\"\n",
    "input_ids = tokenizer(\n",
    "    start_text + prompt,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\"\n",
    ")[\"input_ids\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T23:26:49.984140Z",
     "start_time": "2024-10-07T23:26:49.980460Z"
    }
   },
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": [
    "input_ids"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T22:07:18.506359400Z",
     "start_time": "2024-02-14T22:07:18.496269900Z"
    }
   },
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('lec4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f8896e7784e8225b092959b39daeac8a4547a814dd1ac041b53cb1f9278093e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
